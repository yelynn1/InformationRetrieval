{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import tkinter\n",
    "import sys\n",
    "from pympler import asizeof\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "\n",
    "# !pip install pympler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 List the Files in the specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFile(d):    \n",
    "    path = [os.path.abspath(os.path.join(d,i)) for i in os.listdir(d)]\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read the file and return the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(d):\n",
    "    file = open(d,\"r\",encoding='utf-8')\n",
    "    content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unoptimized (input - content, document number)\n",
    "def createToken(content, dnum):\n",
    "    tokens = list()\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for t in tokenizer.tokenize(content):\n",
    "        tokens = tokens + [[t,dnum]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized (input takes only content)\n",
    "def createTokenOptimized(content):\n",
    "    tokens = list()\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for token in tokenizer.tokenize(content):\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linguistic (lower, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguisticToken(token_list):\n",
    "    stemmer = PorterStemmer() #stem\n",
    "    #lemmer = WordNetLemmatizer()\n",
    "    tokens = list()\n",
    "    regex = \"[!@#$%^&*()-_=+'`~ \\\":;|/.,?[]{}<>]\"\n",
    "    for [t,d] in token_list:\n",
    "        token = t.translate(str.maketrans('', '', regex)) #remove punctuations\n",
    "        if token == '': #if the token is only punctuation\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = stemmer.stem(token)\n",
    "        #token = lemmer.lemmatize(token)\n",
    "        tokens += [[token,d]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized (input takes only content)\n",
    "def linguisticTokenOptimized(token_list):\n",
    "    stemmer = PorterStemmer() #stem\n",
    "    #lemmer = WordNetLemmatizer()\n",
    "    tokens = list()\n",
    "    regex = \"[!@#$%^&*()-_=+'`~ \\\":;|/.,?[]{}<>]\"\n",
    "    for t in token_list:\n",
    "        token = t.translate(str.maketrans('', '', regex)) #remove punctuations\n",
    "        if token == '': #if the token is only punctuation\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = stemmer.stem(token)\n",
    "        #token = lemmer.lemmatize(token)\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unoptimized (sort by term, document id)\n",
    "def sortToken(token_list):\n",
    "\n",
    "    token_list.sort(key=lambda e: (e[0],e[1]))\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by term only\n",
    "def sortTokenFirstOpt(token_list):\n",
    "    token_list.sort(key=lambda e: e[0])\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by term and append docID\n",
    "def sortTokenOptimized(listStr):\n",
    "\n",
    "    newList=[]\n",
    "    for doc in listStr:\n",
    "        for term in doc[0]:\n",
    "            newList.append([term, doc[1]])\n",
    "    newList.sort(key = lambda x: x[0])\n",
    "\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform into posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformPosting(sorted_list):\n",
    "    postDictionary = {}\n",
    "    for term,docId in sorted_list: #add terms into posting\n",
    "        postDictionary.setdefault(term,[]).append(docId)\n",
    "    for key in postDictionary:\n",
    "        post = list(dict.fromkeys(postDictionary[key]))\n",
    "        post.sort(key=str)\n",
    "        postDictionary[key] = [len(post),post]\n",
    "    return postDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformPostingOptimized(sorted_list):\n",
    "    postDictionary = {}\n",
    "    for term,docId in sorted_list: #add terms into posting\n",
    "        postDictionary.setdefault(term,[]).append(docId)\n",
    "    for key in postDictionary:\n",
    "        post = list(dict.fromkeys(postDictionary[key]))\n",
    "        post.sort(key=int)\n",
    "        postDictionary[key] = [len(post),post]\n",
    "    return postDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Merge the Postings (Intersecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of the algorithm from lecutre (for AND operations)\n",
    "def mergePostings(postingList):\n",
    "    result = []\n",
    "    posting1 = postingList[0]\n",
    "    \n",
    "    for i in range(1,len(postingList),1):\n",
    "        merged = []\n",
    "        posting2 = postingList[i]\n",
    "        p = 0\n",
    "        q = 0\n",
    "        while p < len(posting1) and q < len(posting2):\n",
    "            if int(posting1[p]) == int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "                q += 1\n",
    "            elif int(posting1[p]) < int(posting2[q]):\n",
    "                p += 1\n",
    "            else:\n",
    "                q += 1\n",
    "        posting1 = merged\n",
    "    return posting1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Postings (for OR operations) - Not in Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for OR operations\n",
    "def mergeOrPostings(postingList):\n",
    "    posting1 = postingList[0]\n",
    "    \n",
    "    for i in range(1,len(postingList),1):\n",
    "        merged = []\n",
    "        posting2 = postingList[i]\n",
    "        p = 0\n",
    "        q = 0\n",
    "        while p < len(posting1) and q < len(posting2):\n",
    "            if int(posting1[p]) == int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "                q += 1\n",
    "            elif int(posting1[p]) < int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "            else:\n",
    "                merged.append(posting2[q])\n",
    "                q += 1\n",
    "        if p < len(posting1):\n",
    "            merged += posting1[p:]\n",
    "        elif q < len(posting2):\n",
    "            merged += posting2[q:]\n",
    "        posting1 = merged\n",
    "    return posting1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inverted index for the directory\n",
    "def non_optimized(directory):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Reading files from \", directory)\n",
    "    files = listFile(directory) #list dir\n",
    "    tokens = list()\n",
    "    num=0\n",
    "    did={}\n",
    "\n",
    "    for docs in tqdm(files):\n",
    "        try:\n",
    "            file_content = readFile(docs) #read content\n",
    "            token = createToken(file_content,num) #create token\n",
    "            token = linguisticToken(token) #stemming\n",
    "            tokens += token\n",
    "            did[num]=docs\n",
    "            num+=1\n",
    "        except:\n",
    "            print(docs)\n",
    "        \n",
    "    \n",
    "    print(\"Sorting the tokens ...\")\n",
    "    tokens = sortToken(tokens) #token from all files\n",
    "\n",
    "    print(\"Transforming into postings ...\")\n",
    "    posting = transformPostingOptimized(tokens) #create posting from these files\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_to_index = end_time - start_time\n",
    "    print(\"Finished indexing.\\nTotal Time taken to index: \" , round(time_to_index,3))\n",
    "    return posting, did, time_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inverted index for the directory\n",
    "def first_optimized(directory):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Reading files from \", directory)\n",
    "    files = listFile(directory) #list dir\n",
    "    tokens = list()\n",
    "    num = 0\n",
    "    did = {}\n",
    "\n",
    "    for docs in tqdm(files):\n",
    "                \n",
    "        try:\n",
    "            file_content = readFile(docs) #read content\n",
    "            token = createToken(file_content,num) #create token\n",
    "            token = linguisticToken(token) #stemming\n",
    "            tokens += token\n",
    "            did[num] = docs\n",
    "            num +=1\n",
    "        except:\n",
    "            print(docs)\n",
    "\n",
    "    print(\"Sorting the tokens ...\")\n",
    "    tokens = sortTokenFirstOpt(tokens) #token from all files\n",
    "\n",
    "    \n",
    "    print(\"Transforming into postings ...\")\n",
    "    posting = transformPostingOptimized(tokens) #create posting from these files\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_to_index = end_time - start_time\n",
    "    print(\"Finished indexing.\\nTotal Time taken to index: \" , round(time_to_index,3))\n",
    "    return posting, did, time_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized inverted index\n",
    "def optimized(directory):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Reading files from \" , directory)\n",
    "    files = listFile(directory) #list dir\n",
    "    tokens = list()\n",
    "    num = 0\n",
    "    did = {}\n",
    "    \n",
    "\n",
    "    for docs in tqdm(files):\n",
    "        try:\n",
    "            file_content = readFile(docs) #read content\n",
    "            token = createTokenOptimized(file_content) #create token\n",
    "            token = linguisticTokenOptimized(token) #stemming\n",
    "            token_id = [token,num]\n",
    "            tokens += [token_id]\n",
    "            did[num] = docs\n",
    "            num +=1\n",
    "        except:\n",
    "            print(docs)\n",
    "        \n",
    "    print(\"Sorting the tokens ...\")\n",
    "    tokens = sortTokenOptimized(tokens) #token from all files\n",
    "    \n",
    "    print(\"Transforming into postings ...\")\n",
    "    posting = transformPostingOptimized(tokens)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_to_index = end_time - start_time\n",
    "    print(\"Finished indexing.\\nTotal Time taken to index: \" , round(time_to_index,3))\n",
    "    return posting, did, time_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Query (transform into token, search, merge and return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery(query):\n",
    "    q_token = createTokenOptimized(query)\n",
    "    q_token = linguisticTokenOptimized(q_token)\n",
    "    posting_list = []\n",
    "    \n",
    "    for token in q_token:\n",
    "        try:\n",
    "            posting_list.append(posting[token][1])\n",
    "        except:\n",
    "            print(token)\n",
    "            posting_list.append([])\n",
    "            break\n",
    "    result = mergePostings(posting_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the indexer (Unoptimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from  datafull-lean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 47938/47938 [35:40<00:00, 33.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the tokens ...\n",
      "Transforming into postings ...\n",
      "Finished indexing.\n",
      "Total Time taken to index:  2600.666\n",
      "Size of index:  628.085488 MB. \n",
      "Size of dictionary 15.494416 MB\n"
     ]
    }
   ],
   "source": [
    "posting, docId, time_to_index = non_optimized(\"datafull-lean\")\n",
    "print(\"Size of index: \", asizeof.asizeof(posting)/1000000,\"MB. \\nSize of dictionary\", asizeof.asizeof(docId)/1000000, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the indexer (Optimized - Single Sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from  datafull-lean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 47938/47938 [34:08<00:00, 23.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the tokens ...\n",
      "Transforming into postings ...\n",
      "Finished indexing.\n",
      "Total Time taken to index:  2140.685\n",
      "Size of index:  628.085488 MB. \n",
      "Size of dictionary 15.494416 MB\n"
     ]
    }
   ],
   "source": [
    "posting, docId, time_to_index = first_optimized(\"datafull-lean\")\n",
    "print(\"Size of index: \", asizeof.asizeof(posting1)/1000000,\"MB. \\nSize of dictionary\", asizeof.asizeof(docId)/1000000, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the indexer (Optimized Appending of DocID and Single Sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from  datafull-lean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 47938/47938 [22:29<00:00, 35.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the tokens ...\n",
      "Transforming into postings ...\n",
      "Finished indexing.\n",
      "Total Time taken to index:  1470.042\n",
      "Size of index:  628.085488 MB. \n",
      "Size of dictionary 15.494416 MB\n"
     ]
    }
   ],
   "source": [
    "posting, docId, time_to_index = optimized(\"datafull-lean\")\n",
    "print(\"Size of index: \", asizeof.asizeof(posting2)/1000000,\"MB. \\nSize of dictionary\", asizeof.asizeof(docId)/1000000, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final IR System Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input(\"Enter a query (type q to exit) : \")\n",
    "    if query == \"q\":\n",
    "        break\n",
    "    clear_output()\n",
    "    start_time = time.time()\n",
    "    result = processQuery(query)\n",
    "    \n",
    "    print( len(result), \"documents found for '\"+query+\"'\")\n",
    "    print(\"Time taken to search: \", time.time()-start_time)\n",
    "    \n",
    "    for doc in result:\n",
    "        print(os.path.basename(docId[int(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the posting list, docID and time_to_index as backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(posting, open(\"backup/posting.p\",\"wb\"))\n",
    "# pickle.dump(docId,open(\"backup/docId.p\",\"wb\"))\n",
    "# pickle.dump(time_to_index,open(\"backup/time_to_index.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the posting list, docID and time_to_index from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting=pickle.load(open(\"backup/posting.p\",\"rb\"))\n",
    "docId=pickle.load(open(\"backup/docId.p\",\"rb\"))\n",
    "time_to_index=pickle.load(open(\"backup/time_to_index.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes for Size Optimization\n",
    "## Includes:\n",
    "- Codes required for Variable Bytes Compression on posting lists\n",
    "- Dictionary-as-a-String Compression for Terms\n",
    "- Combination of Dictionary-as-a-String Compression and Variable Bytes Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes requied for variable bytes compression for posting list\n",
    "\n",
    "def encodeNumber(n):\n",
    "    bytes_list = []\n",
    "    while True:\n",
    "        bytes_list.insert(0, n % 128)\n",
    "        if n < 128:\n",
    "            break\n",
    "        n = n // 128\n",
    "    bytes_list[-1] += 128\n",
    "    return bytearray(bytes_list)\n",
    "    \n",
    "def encode(numbers):\n",
    "    bytes_list = []\n",
    "    for num in numbers:\n",
    "        bytes_list.append(encodeNumber(num))\n",
    "    return b\"\".join(bytes_list)\n",
    "    \n",
    "def decode(bytes_list):\n",
    "    numbers = []\n",
    "    n = 0\n",
    "    for byte in bytes_list:\n",
    "        if byte < 128:\n",
    "            n = 128 * n + byte\n",
    "        else:\n",
    "            n = 128 * n + (byte - 128)\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers\n",
    "\n",
    "def variableBytesCompression(posting):\n",
    "    ptr = {}\n",
    "    for key in tqdm_notebook(posting):\n",
    "\n",
    "        post = posting[key][1]\n",
    "        docfreq = posting[key][0]\n",
    "        \n",
    "        temp_post = []\n",
    "        for docId in range(len(post)):\n",
    "            if(docId == 0):\n",
    "                temp_post.append(post[0])\n",
    "            else:\n",
    "                temp_post.append(post[docId]-post[docId-1])\n",
    "\n",
    "        ptr[key] = [docfreq,encode(temp_post)]\n",
    "    return ptr\n",
    "\n",
    "def search_variableBytesCompression(ptr,t):\n",
    "    key = t\n",
    "    post = decode(ptr[key][1])\n",
    "    temp = [post[0]]\n",
    "    for i in range(1,len(post),1):\n",
    "        temp.append(post[i]+temp[i-1])\n",
    "    result = [ptr[key][0],temp]\n",
    "    return result\n",
    "\n",
    "def dict_as_string(posting):\n",
    "    longstr = \"\";\n",
    "    ptr = {}\n",
    "    i = 0\n",
    "    for key in tqdm_notebook(posting):\n",
    "        \n",
    "        i+=len(key)\n",
    "        longstr += key\n",
    "\n",
    "        post = posting[key][1]\n",
    "        docfreq = posting[key][0]\n",
    "\n",
    "        ptr[i] = [docfreq,post]\n",
    "    return longstr,ptr\n",
    "\n",
    "def search_dict_as_string(longstr,ptr,t):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    i = 0\n",
    "    for key in ptr:\n",
    "        end = key\n",
    "        \n",
    "        word = longstr[start:end]\n",
    "        if(t == word):\n",
    "            return ptr[key]\n",
    "            break\n",
    "            \n",
    "        start = end\n",
    "        \n",
    "def combined_compress(posting):\n",
    "    longstr = \"\";\n",
    "    ptr = {}\n",
    "    i = 0\n",
    "    for key in tqdm_notebook(posting):        \n",
    "        i+=len(key)\n",
    "        longstr += key\n",
    "\n",
    "        post = posting[key][1]\n",
    "        docfreq = posting[key][0]\n",
    "        \n",
    "        temp_post = []\n",
    "        for docId in range(len(post)):\n",
    "            if(docId == 0):\n",
    "                temp_post.append(post[0])\n",
    "            else:\n",
    "                temp_post.append(post[docId]-post[docId-1])\n",
    "\n",
    "        ptr[i] = [docfreq,encode(temp_post)]\n",
    "    return longstr,ptr\n",
    "\n",
    "def combined_search(longstr,ptr,t):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    i = 0\n",
    "    for key in ptr:\n",
    "        end = key\n",
    "        \n",
    "        word = longstr[start:end]\n",
    "        if(t == word):\n",
    "            post = decode(ptr[key][1])\n",
    "            temp = [post[0]]\n",
    "            for i in range(1,len(post),1):\n",
    "                temp.append(post[i]+temp[i-1])\n",
    "            result = [ptr[key][0],temp]\n",
    "            return result\n",
    "            break\n",
    "            \n",
    "        start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c76461e6e348e3934d815f8ffa04bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1477855), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Variable Bytes Compression: 382.797928 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b483daf5b64e3280e1fdf998fb5320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1477855), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Dict-as-String Compression: 576.971088 MB\n",
      "Dictionary String Size: 55.763536 MB\n",
      "Total Size: 632.734624 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bac072123ad440b9966b776b0797c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1477855), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after Dict-as-String Compression: 331.683528 MB\n",
      "Dictionary String Size: 55.763536 MB\n",
      "Total Size: 387.447064 MB\n"
     ]
    }
   ],
   "source": [
    "# Variable Bytes Compression on Posting List\n",
    "ptrVariableBytes = variableBytesCompression(posting) \n",
    "print('Size after Variable Bytes Compression:',asizeof.asizeof(ptrVariableBytes)/1000000,'MB')\n",
    "\n",
    "# Dict-as-String Compression for Terms\n",
    "longstr,ptrDaS= dict_as_string(posting)\n",
    "print('Size after Dict-as-String Compression:',asizeof.asizeof(ptrDaS)/1000000,'MB')\n",
    "print('Dictionary String Size:',asizeof.asizeof(longstr)/1000000,'MB')\n",
    "print('Total Size:',asizeof.asizeof(longstr)/1000000+asizeof.asizeof(ptrDaS)/1000000,'MB')\n",
    "\n",
    "# Combination of Dic-as-String and Variable Bytes Compression\n",
    "longstr2,ptrCombined = combined_compress(posting)\n",
    "print('Size after Dict-as-String Compression:',asizeof.asizeof(ptrCombined)/1000000,'MB')\n",
    "print('Dictionary String Size:',asizeof.asizeof(longstr2)/1000000,'MB')\n",
    "print('Total Size:',asizeof.asizeof(longstr2)/1000000+asizeof.asizeof(ptrCombined)/1000000,'MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
