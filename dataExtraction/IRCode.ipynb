{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#docs = ['why hello there', 'omg hello pony', 'she went there? omg']\n",
    "def termDocumentMatrix(docs):\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(docs)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>bob</th>\n",
       "      <th>brothers</th>\n",
       "      <th>closed</th>\n",
       "      <th>john</th>\n",
       "      <th>store</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>was</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  are  bob  brothers  closed  john  store  the  to  too  was  went\n",
       "0    1    1    1         1       0     1      0    0   0    0    0     0\n",
       "1    0    0    0         0       1     1      2    2   1    0    1     1\n",
       "2    0    0    1         0       0     0      1    1   1    1    0     1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = ['John and Bob are brothers.','John went to the store. The store was closed.','Bob went to the store too.']\n",
    "termDocumentMatrix(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "def termMatrix(doc):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vec = vectorizer.fit_transform(doc)\n",
    "    df2 = pd.DataFrame(doc_vec.toarray().transpose(),\n",
    "                   index=vectorizer.get_feature_names())\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>are</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bob</td>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>brothers</td>\n",
       "      <td>0.490479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>closed</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345808</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0.373022</td>\n",
       "      <td>0.262996</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>store</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>0.385503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>0.385503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>to</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262996</td>\n",
       "      <td>0.385503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>too</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>was</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345808</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>went</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262996</td>\n",
       "      <td>0.385503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2\n",
       "and       0.490479  0.000000  0.000000\n",
       "are       0.490479  0.000000  0.000000\n",
       "bob       0.373022  0.000000  0.385503\n",
       "brothers  0.490479  0.000000  0.000000\n",
       "closed    0.000000  0.345808  0.000000\n",
       "john      0.373022  0.262996  0.000000\n",
       "store     0.000000  0.525991  0.385503\n",
       "the       0.000000  0.525991  0.385503\n",
       "to        0.000000  0.262996  0.385503\n",
       "too       0.000000  0.000000  0.506890\n",
       "was       0.000000  0.345808  0.000000\n",
       "went      0.000000  0.262996  0.385503"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = ['John and Bob are brothers.','John went to the store. The store was closed.','Bob went to the store too.']\n",
    "termMatrix(aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permuterm Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('Stopwords.txt') as f:\n",
    "    for line in f:\n",
    "        stopwords = line.split(\", \")\n",
    "#print stopwords\n",
    "\n",
    "tokens = {}\n",
    "documentID = 0\n",
    "path = r\"C:\\Users\\Phu Wai Paing\\Desktop\\Course Books\\Information Retrieval\\dataExtraction\\dataset\"\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "                documentID += 1\n",
    "                line_tokens = []\n",
    "                for line in f:\n",
    "                    line_tokens = line.split(  )\n",
    "                    for each in line_tokens:\n",
    "                        if each not in stopwords:\n",
    "                            if each not in tokens:\n",
    "                                tokens[each] = [documentID]\n",
    "                            else:\n",
    "                                tokens[each].append(documentID)\n",
    "\n",
    "file = open(\"InvertedIndex.txt\", \"w\")\n",
    "for key in sorted(tokens):\n",
    "    file.write(key)\n",
    "    file.write(\" \")\n",
    "    value = ','.join(str(v) for v in tokens[key])\n",
    "    file.write(value)\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n",
    "\n",
    "def rotate(str, n):\n",
    "    return str[n:] + str[:n]\n",
    "\n",
    "file = open(\"PermutermIndex.txt\",\"w\")\n",
    "keys = tokens.keys()\n",
    "for key in sorted(keys):\n",
    "    dkey = key + \"$\"\n",
    "    for i in range(len(dkey),0,-1):\n",
    "        out = rotate(dkey,i)\n",
    "        file.write(out)\n",
    "        file.write(\" \")\n",
    "        file.write(key)\n",
    "        file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ohell'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotate(\"hello\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query : f*e\n",
      "2\n",
      "case =  3\n",
      "{'big': ['1'], 'bubble': ['3'], 'burst': ['3'], 'data': ['1', '2', '3'], 'future': ['1'], 'linkedin': ['3'], 'mining': ['3'], 'new': ['2'], 'oil': ['2'], 'predicts': ['3'], 'statistics': ['3']}\n",
      "{'big$': 'big', '$big': 'big', 'g$bi': 'big', 'ig$b': 'big', 'bubble$': 'bubble', '$bubble': 'bubble', 'e$bubbl': 'bubble', 'le$bubb': 'bubble', 'ble$bub': 'bubble', 'bble$bu': 'bubble', 'ubble$b': 'bubble', 'burst$': 'burst', '$burst': 'burst', 't$burs': 'burst', 'st$bur': 'burst', 'rst$bu': 'burst', 'urst$b': 'burst', 'data$': 'data', '$data': 'data', 'a$dat': 'data', 'ta$da': 'data', 'ata$d': 'data', 'future$': 'future', '$future': 'future', 'e$futur': 'future', 're$futu': 'future', 'ure$fut': 'future', 'ture$fu': 'future', 'uture$f': 'future', 'linkedin$': 'linkedin', '$linkedin': 'linkedin', 'n$linkedi': 'linkedin', 'in$linked': 'linkedin', 'din$linke': 'linkedin', 'edin$link': 'linkedin', 'kedin$lin': 'linkedin', 'nkedin$li': 'linkedin', 'inkedin$l': 'linkedin', 'mining$': 'mining', '$mining': 'mining', 'g$minin': 'mining', 'ng$mini': 'mining', 'ing$min': 'mining', 'ning$mi': 'mining', 'ining$m': 'mining', 'new$': 'new', '$new': 'new', 'w$ne': 'new', 'ew$n': 'new', 'oil$': 'oil', '$oil': 'oil', 'l$oi': 'oil', 'il$o': 'oil', 'predicts$': 'predicts', '$predicts': 'predicts', 's$predict': 'predicts', 'ts$predic': 'predicts', 'cts$predi': 'predicts', 'icts$pred': 'predicts', 'dicts$pre': 'predicts', 'edicts$pr': 'predicts', 'redicts$p': 'predicts', 'statistics$': 'statistics', '$statistics': 'statistics', 's$statistic': 'statistics', 'cs$statisti': 'statistics', 'ics$statist': 'statistics', 'tics$statis': 'statistics', 'stics$stati': 'statistics', 'istics$stat': 'statistics', 'tistics$sta': 'statistics', 'atistics$st': 'statistics', 'tatistics$s': 'statistics'}\n",
      "['future']\n",
      "[['1']]\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "query = input('Enter Query : ')\n",
    "\n",
    "parts = query.split(\"*\")\n",
    "print(len(parts))\n",
    "\n",
    "#    1. X*               -->         X \n",
    "#\t2. *X\t\t\t\t-->\t\t\tX$*\n",
    "#\t3. X*Y\t\t\t\t-->\t\t\tY$X*\n",
    "#\t4. X*Y*Z\t\t\t-->\t\t\t(Z$X*) and (Y*)\n",
    "#\t5. *X* \t\t\t\t-->\t\t\tcan be converted to X* form\n",
    "    \n",
    "if len(parts) == 3:\n",
    "    case = 4\n",
    "elif parts[1] == '':\n",
    "    case = 1\n",
    "elif parts[0] == '':\n",
    "    case = 2\n",
    "elif parts[0] != '' and parts[1] != '':\n",
    "    case = 3\n",
    "\n",
    "if case == 4:\n",
    "    if parts[0] == '':\n",
    "        case = 1\n",
    "\n",
    "print(\"case = \", case)\n",
    "\n",
    "inverted = {}\n",
    "with open(r'C:\\Users\\Phu Wai Paing\\Desktop\\Course Books\\Information Retrieval\\dataExtraction\\InvertedIndex.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split( )\n",
    "        val = temp[1].split(\",\")\n",
    "        inverted[temp[0]] = val\n",
    "\n",
    "print(inverted)    \n",
    "\n",
    "permuterm = {}\n",
    "with open(r'C:\\Users\\Phu Wai Paing\\Desktop\\Course Books\\Information Retrieval\\dataExtraction\\PermutermIndex.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split( )\n",
    "        permuterm[temp[0]] = temp[1]\n",
    "        \n",
    "print(permuterm)\n",
    "\n",
    "def prefix_match(term, prefix):\n",
    "    term_list = []\n",
    "    for tk in term.keys():\n",
    "        if tk.startswith(prefix):\n",
    "            term_list.append(term[tk])\n",
    "    return term_list\n",
    "        \n",
    "docID = 0\n",
    "\n",
    "def processQuery(query):    \n",
    "    term_list = prefix_match(permuterm,query)\n",
    "    print(term_list)\n",
    "    \n",
    "    docID = []\n",
    "    for term in term_list:\n",
    "        docID.append(inverted[term])\n",
    "    print(docID)\n",
    "\n",
    "    temp = []\n",
    "    for x in docID:\n",
    "        for y in x:\n",
    "            temp.append(y)\n",
    "    print(temp)        \n",
    "\n",
    "    temp = [int(x) for x in temp]\n",
    "    documentID = 0\n",
    "    outputfile = open(\"RetrievedDocuments.txt\",\"w\")\n",
    "    path = r\"C:\\Users\\Phu Wai Paing\\Desktop\\Course Books\\Information Retrieval\\dataExtraction\\dataset\"\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            documentID = documentID + 1\n",
    "            with open(os.path.join(path, file)) as f:\n",
    "                for text in f:\n",
    "                    if documentID in temp:\n",
    "                        outputfile.write(file + \"\\n\" + text + \"\\n\")\n",
    "            f.close()\n",
    "    outputfile.close()\n",
    "    return \n",
    "\n",
    "#    1. X*               -->         X \n",
    "#\t2. *X\t\t\t\t-->\t\t\tX$*\n",
    "#\t3. X*Y\t\t\t\t-->\t\t\tY$X*\n",
    "#\t4. X*Y*Z\t\t\t-->\t\t\t(Z$X*) and (Y*)\n",
    "#\t5. *X* \t\t\t\t-->\t\t\tcan be converted to X* form\n",
    "\n",
    "if case == 1:\n",
    "    query = parts[0]\n",
    "elif case == 2:\n",
    "    query = parts[1] + \"$\"\n",
    "elif case == 3:\n",
    "    query = parts[1] + \"$\" + parts[0]\n",
    "elif case == 4:\n",
    "    queryA = parts[2] + \"$\" + parts[0]\n",
    "    queryB = parts[1]\n",
    "\n",
    "def bitwise_and(A,B):\n",
    "    return set(A).intersection(B)\n",
    "    \n",
    "if case != 4:\n",
    "    processQuery(query)\n",
    "elif case == 4:\n",
    "# queryA Z$X\n",
    "    term_list = prefix_match(permuterm,queryA)\n",
    "    #print(term_list)\n",
    "    \n",
    "    docID = []\n",
    "    for term in term_list:\n",
    "        docID.append(inverted[term])\n",
    "    #print(docID)\n",
    "\n",
    "    temp1 = []\n",
    "    for x in docID:\n",
    "        for y in x:\n",
    "            temp1.append(y)\n",
    "    #print(temp)        \n",
    "\n",
    "    temp1 = [int(x) for x in temp1]\n",
    "# queryB Y\n",
    "    term_list = prefix_match(permuterm,queryB)\n",
    "    #print(term_list)\n",
    "    \n",
    "    docID = []\n",
    "    for term2 in term_list:\n",
    "        docID.append(inverted[term2])\n",
    "    #print(docID)\n",
    "\n",
    "    temp2 = []\n",
    "    for x in docID:\n",
    "        for y in x:\n",
    "            temp2.append(y)\n",
    "    #print(temp)        \n",
    "\n",
    "    temp2 = [int(x) for x in temp2]\n",
    "\n",
    "    temp = bitwise_and(temp1,temp2)\n",
    "\n",
    "  #  print(temp1,temp2,temp)    \n",
    "    documentID = 0\n",
    "    outputfile = open(\"RetrievedDocuments.txt\",\"w\")\n",
    "    path = r\"C:\\Users\\Phu Wai Paing\\Desktop\\Course Books\\Information Retrieval\\dataExtraction\\dataset\"\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            documentID = documentID + 1\n",
    "            with open(os.path.join(path, file)) as f:\n",
    "                for text in f:\n",
    "                    if documentID in temp:\n",
    "                        outputfile.write(file + \"\\n\" + text + \"\\n\")\n",
    "            f.close()\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    " \n",
    "def soundex(word):\n",
    "   codes = (\"bfpv\",\"cgjkqsxz\", \"dt\", \"l\", \"mn\", \"r\")\n",
    "   soundDict = dict((ch, str(ix+1)) for ix,cod in enumerate(codes) for ch in cod)\n",
    "   cmap2 = lambda a: soundDict.get(a, '9')\n",
    "   sdx =  ''.join(cmap2(a) for a in word.lower())\n",
    "   sdx2 = word[0].upper() + ''.join(k for k,g in list(groupby(sdx))[1:] if k!='9')\n",
    "   sdx3 = sdx2[0:4].ljust(4,'0')\n",
    "   return sdx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A515'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex(\"anbmgrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPIMI_Invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import re\n",
    "import collections\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def spimi_invert(documents, block_size_limit):\n",
    "    \"\"\" Applies the Single-pass in-memory indexing algorithm \"\"\"\n",
    "    block_number = 0\n",
    "    documents_count = len(documents)\n",
    "    dictionary = {} # (term - postings list)\n",
    "    for index, docID in enumerate(documents):\n",
    "        for term in documents[docID]:\n",
    "            # If term occurs for the first time\n",
    "            if term not in dictionary:\n",
    "                # Add term to dictionary, create new postings list, and add docID\n",
    "                dictionary[term] = [docID]\n",
    "            # else:\n",
    "            #     # If term has a subsequent occurence\n",
    "            #     if docID not in dictionary[term]:\n",
    "            #         # Add a posting (docID) to the existing posting list of the term\n",
    "            #         dictionary[term].append(docID)\n",
    "            else:\n",
    "                dictionary[term].append(docID)\n",
    "        if sys.getsizeof(dictionary) > block_size_limit or (index == documents_count-1):\n",
    "            temp_dict = sort_terms(dictionary)\n",
    "            write_block_to_disk(temp_dict, block_number)\n",
    "            temp_dict = {}\n",
    "            block_number += 1\n",
    "            dictionary = {}\n",
    "    print(\"SPIMI invert complete!\")\n",
    "\n",
    "def sort_terms(term_postings_list):\n",
    "    \"\"\" Sorts dictionary terms in alphabetical order \"\"\"\n",
    "    print(\" -- Sorting terms...\")\n",
    "    sorted_dictionary = OrderedDict() # keep track of insertion order\n",
    "    sorted_terms = sorted(term_postings_list)\n",
    "    for term in sorted_terms:\n",
    "        result = [int(docIds) for docIds in term_postings_list[term]]\n",
    "        result_tftd = calculate_tftd(result)\n",
    "        sorted_dictionary[term] = result_tftd\n",
    "    return sorted_dictionary\n",
    "\n",
    "def calculate_tftd(pl_with_duplicates):\n",
    "    \"\"\" Add term frequency of term in each document \"\"\"\n",
    "    # print(pl_with_duplicates)\n",
    "    counter = collections.Counter(pl_with_duplicates)\n",
    "    pl_tftd = [[int(docId), counter[docId]] for docId in counter.keys()]\n",
    "    return pl_tftd\n",
    "\n",
    "def write_block_to_disk(term_postings_list, block_number):\n",
    "    \"\"\" Writes index of the block (dictionary + postings list) to disk \"\"\"\n",
    "    # Define block\n",
    "    base_path = 'index_blocks/'\n",
    "    block_name = 'block-' + str(block_number) + '.txt'\n",
    "    block = open(base_path + block_name, 'a+')\n",
    "    print(\" -- Writing term-positing list block: \" + block_name + \"...\")\n",
    "    # Write term : posting lists to block\n",
    "    for index, term in enumerate(term_postings_list):\n",
    "        # Term - Posting List Format\n",
    "        # term:[docID1, docID2, docID3]\n",
    "        # e.g. cat:[4,9,21,42]\n",
    "        block.write(str(term) + \":\" + str((term_postings_list[term])) + \"\\n\")\n",
    "    block.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
