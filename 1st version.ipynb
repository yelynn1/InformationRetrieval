{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import tkinter\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 List the Files in the specific directory (Section 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFile(d):    \n",
    "    path = [os.path.abspath(os.path.join(d,i)) for i in os.listdir(d)]\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read the file and return the content (Section 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(d):\n",
    "    file = open(d,\"r\",encoding='utf-8')\n",
    "    content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create token (Section 1)\n",
    "- input (content, document id)\n",
    "- output (pairs of token and document id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unoptimized (input - content, document number)\n",
    "def createToken(content, dnum):\n",
    "    tokens = list()\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for t in tokenizer.tokenize(content):\n",
    "        tokens = tokens + [[t,dnum]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized (input takes only content)\n",
    "def createTokenOptimized(content):\n",
    "    tokens = list()\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for token in tokenizer.tokenize(content):\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linguistic (lower, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguisticToken(token_list):\n",
    "    stemmer = PorterStemmer() #stem\n",
    "    #lemmer = WordNetLemmatizer()\n",
    "    tokens = list()\n",
    "    regex = \"[!@#$%^&*()-_=+'`~ \\\":;|/.,?[]{}<>]\"\n",
    "    for [t,d] in token_list:\n",
    "        token = t.translate(str.maketrans('', '', regex)) #remove punctuations\n",
    "        if token == '': #if the token is only punctuation\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = stemmer.stem(token)\n",
    "        #token = lemmer.lemmatize(token)\n",
    "        tokens += [[token,d]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linguisticTokenOptimized(token_list):\n",
    "    stemmer = PorterStemmer() #stem\n",
    "    #lemmer = WordNetLemmatizer()\n",
    "    tokens = list()\n",
    "    regex = \"[!@#$%^&*()-_=+'`~ \\\":;|/.,?[]{}<>]\"\n",
    "    for t in token_list:\n",
    "        token = t.translate(str.maketrans('', '', regex)) #remove punctuations\n",
    "        if token == '': #if the token is only punctuation\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        token = stemmer.stem(token)\n",
    "        #token = lemmer.lemmatize(token)\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unoptimized (sort by term, document id)\n",
    "def sortToken(token_list):\n",
    "    token_list.sort(key=lambda e: (e[0],e[1]))\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized (sort only by term)\n",
    "def sortTokenOptimized(listStr):\n",
    "    newList=[]\n",
    "    for i in range(len(listStr)):\n",
    "        for j in range(len(listStr[i][0])):\n",
    "            newList.append([listStr[i][0][j],i])\n",
    "    sortedToken = sorted(newList, key = lambda x: x[0])\n",
    "    return sortedToken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transform into posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformPosting(sorted_list):\n",
    "    postDictionary = {}\n",
    "    for term,docId in sorted_list: #add terms into posting\n",
    "        postDictionary.setdefault(term,[]).append(docId)\n",
    "    for key in postDictionary:\n",
    "        post = list(dict.fromkeys(postDictionary[key]))\n",
    "        post.sort(key=int)\n",
    "        postDictionary[key] = [len(post),post]\n",
    "    return postDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Merge the Postings (Intersecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of the algorithm from lecutre (for AND operations)\n",
    "def mergePostings(postingList):\n",
    "    result = []\n",
    "    posting1 = postingList[0]\n",
    "    \n",
    "    for i in range(1,len(postingList),1):\n",
    "        merged = []\n",
    "        posting2 = postingList[i]\n",
    "        p = 0\n",
    "        q = 0\n",
    "        while p < len(posting1) and q < len(posting2):\n",
    "            if int(posting1[p]) == int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "                q += 1\n",
    "            elif int(posting1[p]) < int(posting2[q]):\n",
    "                p += 1\n",
    "            else:\n",
    "                q += 1\n",
    "        posting1 = merged\n",
    "    return posting1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge (for OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for OR operations\n",
    "def mergeOrPostings(postingList):\n",
    "    posting1 = postingList[0]\n",
    "    \n",
    "    for i in range(1,len(postingList),1):\n",
    "        merged = []\n",
    "        posting2 = postingList[i]\n",
    "        p = 0\n",
    "        q = 0\n",
    "        while p < len(posting1) and q < len(posting2):\n",
    "            if int(posting1[p]) == int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "                q += 1\n",
    "            elif int(posting1[p]) < int(posting2[q]):\n",
    "                merged.append(posting1[p])\n",
    "                p += 1\n",
    "            else:\n",
    "                merged.append(posting2[q])\n",
    "                q += 1\n",
    "        if p < len(posting1):\n",
    "            merged += posting1[p:]\n",
    "        elif q < len(posting2):\n",
    "            merged += posting2[q:]\n",
    "        posting1 = merged\n",
    "    return posting1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the synnonnumys of a term\n",
    "def getSynon(word):\n",
    "    syns = wn.synsets(word)\n",
    "    synslist = [i.lemmas()[0].name() for i in syns] #put evely synnonnyms to list\n",
    "    synslist = list(dict.fromkeys(synslist)) #remove duplicates\n",
    "    return synslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inverted index for the directory\n",
    "def non_optimized(directory):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Reading files from \", directory)\n",
    "    files = listFile(directory) #list dir\n",
    "    tokens = list()\n",
    "    num = 0\n",
    "    did = {}\n",
    "    for docs in tqdm(files):\n",
    "        try:\n",
    "            file_content = readFile(docs) #read content\n",
    "            token = createToken(file_content,num) #create token\n",
    "            token = linguisticToken(token) #stemming\n",
    "            tokens += token\n",
    "            did[num] = docs\n",
    "            num +=1\n",
    "        except:\n",
    "            print(docs)\n",
    "    \n",
    "    print(\"Sorting the tokens ...\")\n",
    "    tokens = sortToken(tokens) #token from all files\n",
    "    \n",
    "    print(\"Transforming into postings ...\")\n",
    "    posting = transformPosting(tokens) #create posting from these files\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_to_index = end_time - start_time\n",
    "    print(\"Finished indexing.\\nTime taken to index: \" , round(time_to_index,3))\n",
    "    return posting, did, time_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized inverted index\n",
    "def optimized(directory):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Reading files from \" , directory)\n",
    "    files = listFile(directory) #list dir\n",
    "    tokens = list()\n",
    "    num = 0\n",
    "    did = {}\n",
    "    for docs in tqdm(files):\n",
    "        #print(docs,num)\n",
    "        try:\n",
    "            file_content = readFile(docs) #read content\n",
    "            token = createTokenOptimized(file_content) #create token\n",
    "            token = linguisticTokenOptimized(token) #stemming\n",
    "            token_id = [token,num]\n",
    "            tokens += [token_id]\n",
    "            did[num] = docs\n",
    "            num +=1\n",
    "        except:\n",
    "            print(docs)\n",
    "    \n",
    "    print(\"Sorting the tokens ...\")\n",
    "    tokens = sortTokenOptimized(tokens) #token from all files\n",
    "    \n",
    "    print(\"Transforming into postings ...\")\n",
    "    posting = transformPosting(tokens)\n",
    "    end_time = time.time()\n",
    "    time_to_index = end_time - start_time\n",
    "    print(\"Finished indexing.\\nTime taken to index: \" , round(time_to_index,3))\n",
    "    return posting, did, time_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Query (transform into token, search, merge and return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery(query):\n",
    "    q_token = createTokenOptimized(query)\n",
    "    q_token = linguisticTokenOptimized(q_token)\n",
    "    posting_list = []\n",
    "    \n",
    "    for token in q_token:\n",
    "        try:\n",
    "            posting_list.append(posting[token][1])\n",
    "        except:\n",
    "            print(token)\n",
    "            posting_list.append([])\n",
    "            break\n",
    "    result = mergePostings(posting_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the indexer (Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from  HillaryEmails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7945/7945 [01:05<00:00, 121.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the tokens ...\n",
      "Transforming into postings ...\n",
      "Finished indexing.\n",
      "Time taken to index:  70.536\n",
      "Size of index:  2621544  bytes\n"
     ]
    }
   ],
   "source": [
    "posting, docId, time_to_index = non_optimized(\"HillaryEmails\")\n",
    "print(\"Size of index: \", sys.getsizeof(posting), \" bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the indexer (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files from  HillaryEmails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7945/7945 [00:53<00:00, 148.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting the tokens ...\n",
      "Transforming into postings ...\n",
      "Finished indexing.\n",
      "Time taken to index:  59.297\n",
      "Size of index:  2621544  bytes\n"
     ]
    }
   ],
   "source": [
    "posting, docId, time_to_index = optimized(\"HillaryEmails\")\n",
    "print(\"Size of index: \", sys.getsizeof(posting), \" bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Search Engine (without GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 documents found for  place where good foods are available\n",
      "Time taken to search:  0.009974479675292969\n",
      "1541.txt\n",
      "1893.txt\n",
      "2561.txt\n",
      "4212.txt\n",
      "4292.txt\n",
      "5490.txt\n",
      "5789.txt\n",
      "6037.txt\n",
      "6258.txt\n",
      "6478.txt\n",
      "6707.txt\n",
      "6708.txt\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"Enter a query (type q to exit) : \")\n",
    "    if query == \"q\":\n",
    "        break\n",
    "    clear_output()\n",
    "    start_time = time.time()\n",
    "    result = processQuery(query)\n",
    "    \n",
    "    print( len(result), \"documents found for \", query)\n",
    "    print(\"Time taken to search: \", time.time()-start_time)\n",
    "    \n",
    "    for doc in result:\n",
    "        print(os.path.basename(docId[int(doc)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Search Engine (with GUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = tkinter.Tk()\n",
    "\n",
    "def processBtn():\n",
    "    start_time = time.time()\n",
    "    query = inputEntry.get()\n",
    "    result = processQuery(query)\n",
    "    toShow = str(len(result)) + \" documents found for \" + query + \"\\n\"\n",
    "    toShow += \"Time taken to search: \" + str(time.time()-start_time) +\"\\n\"\n",
    "    rs.config(text = toShow)\n",
    "    rsFiles = \"\"\n",
    "    for num, doc in enumerate(result):\n",
    "        rsFiles += str(num) + \". \" + os.path.basename(docId[int(doc)]) + \"\\n\"\n",
    "    rsdetail.config(text = rsFiles)  \n",
    "\n",
    "window.title(\"NTU Search Engine\")\n",
    "window.geometry(\"600x1024\")\n",
    "label = tkinter.Label(window, text = \"Welcome to our Search Engine\", width = 60, fg=\"red\")\n",
    "label.grid(row = 0)\n",
    "inputEntry = tkinter.Entry(window,text = \"Type here to search\", width= 60)\n",
    "inputEntry.grid(row=1, column = 0)\n",
    "button_widget = tkinter.Button(window,text=\"Search\", command = processBtn)\n",
    "button_widget.grid(row=1, column = 1)\n",
    "rs = tkinter.Label(window, text=\"\", fg=\"red\")\n",
    "rs.grid(row=3, column = 0, columnspan=2)\n",
    "\n",
    "rsdetail = tkinter.Message(window, bg=\"white\")\n",
    "rsdetail.grid(row=4, column = 0, columnspan=2, sticky = \"w\")\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the time comparison between Normal and Optimized (average of 10 runs with data sample of 100 files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal:  2.983490324020386 seconds average\n",
      "Optimized:  2.357524847984314 seconds average\n",
      "26.551808205596345 % faster\n"
     ]
    }
   ],
   "source": [
    "time_normal = 0\n",
    "time_opt = 0\n",
    "for i in range(10):\n",
    "    print(\"Running \", i, \" iteration for optimized one\")\n",
    "    pO, dO, time_to_index_opt = optimized(\"data_sample\")\n",
    "    time_opt += time_to_index_opt\n",
    "    clear_output()\n",
    "    print(\"Running \", i, \" iteration for normal one\")\n",
    "    p, d, time_to_index_normal = non_optimized(\"data_sample\")\n",
    "    time_normal += time_to_index_normal\n",
    "    clear_output()\n",
    "print(\"Normal: \" , time_normal/10, \"seconds average\")\n",
    "print(\"Optimized: \", time_opt/10, \"seconds average\")\n",
    "print(((time_normal-time_opt)/time_opt)*100, \"% faster\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
